# Globus

Globus is widely used by HPC systems to transfer data between systems.

It has a concept of a [research data portal](https://docs.globus.org/guides/recipes/modern-research-data-portal/) which is a design pattern for providing secure and high performance access to research data.

## Subscriptions

Globus offers a free subscription tier for limited control of data transfer.  However, higher performance transfers are available with a paid subscription.

There is potentially some issues related to commercial companies having University accounts (e.g. spin-offs and close collaborators) who would then have access to Globus through a University subscription.  The details here need to be ascertained.  Similarly, how the transfer of data to commercial partners can be managed needs to be determined.

## COSMA/ARCHER2 Transfer Tests

In late October and early November 2025, various data transfer tests were performed between COSMA in Durham and ARCHER2 in Edinburgh, using Globus. System administrators in Edinburgh planned to enable MTU 9000 (so-called "Jumbo" ethernet frames) and were interested to see whether this would make a difference to the observed transfer rates in and out of the site. Durham, which already had jumbo frames enabled, was identified as a good candidate for testing and this also fitted well with the data federation project's goal of investigating different data transfer technologies.

Set up was straightforward as both COSMA and ARCHER2 were already registered as Globus endpoints. Before the first use of these endpoints it was necessary to perform an authorisation step, and for COSMA it was also necessary to submit a ticket to the COSMA helpdesk asking for the desired directory to be added to the list of directories visible to Globus. As there was no suitable test data available at either site, random data files were generated by using the `dd` utility to copy random data from `/dev/urandom`. Using random data ensures that the files will not be compressible so that the reported transfer rates will be accurate.

The table below shows the data transfers performed and the transfer rates achieved:

| Date/time | Source | Destination | Number of files | Size of each file (GB) | Transfer rate (MB/s) |
| ----------- | ----------- | ----------- | ----------- | ----------- | ----------- |
| 29/10/2025 11:50 | COSMA | ARCHER2 | 50 | 100 | 815.08 |
| 31/10/2025 22:05 | ARCHER2 | COSMA | 50 | 100 | 916.13 |
| 05/11/2025 07:40 | COSMA | ARCHER2 | 500 | 10 | 970.24 |
| 05/11/2025 12:44 | ARCHER2 | COSMA | 500 | 10 | 994.09 |
| 12/11/2025 09:45 | COSMA | ARCHER2 | 500 | 10 | 914.34 |
| 12/11/2025 11:23 | ARCHER2 | COSMA | 500 | 10 | 944.99 |
| 13/11/2025 07:08 | COSMA | ARCHER2 | 50 | 100 | 860.15 |
| 13/11/2025 09:29 | ARCHER2 | COSMA | 50 | 100 | 817.52 |

The first four transfers were performed before MTU 9000 was enabled at Edinburgh, the last four after. There was no obvious performance improvement from this configuration change, with the transfers both before and after running at similar speeds in the range 800-1000MB/s. It is likely that other bottlenecks in the network were more significant than this setting.

Transfers from ARCHER2 to COSMA were generally slightly faster than those in the other direction, though given the relatively small number of transfers performed, this may have just been coincidence. Surprisingly the transfers of 500 smaller files were noticeably faster than those of 50 larger files; this may relate to how Globus handles transfers internally. Possibly the larger number of files offers more opportunity for parallelism.
